{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVioyXJSKQJM"
      },
      "source": [
        "#@title Run this to download data and prepare the environment.\n",
        "! wget -O data.csv 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Fairness/data.csv' &> /dev/null\n",
        "! wget -O metric_diagrams.zip 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Fairness/metric_diagrams.zip' &> /dev/null\n",
        "! unzip -oq metric_diagrams.zip\n",
        "! pip install aif360 fairlearn==0.4.6 &> /dev/null\n",
        "\n",
        "from IPython.display import Image, display, Markdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.templates.default = \"plotly_white\"\n",
        "\n",
        "SKILLS = [\n",
        "    \"Java\",\n",
        "    \"Python\",\n",
        "    \"Recruiting\",\n",
        "    \"Web_Development\",\n",
        "    \"Databases\",\n",
        "    \"Machine_Learning\",\n",
        "    \"Materials\",\n",
        "    \"AutoCAD\",\n",
        "    \"Data_Science\",\n",
        "    \"Art\",\n",
        "    \"Design\",\n",
        "    \"Marketing\",\n",
        "    \"Finance\",\n",
        "    \"Accounting\",\n",
        "    \"Writing\",\n",
        "    \"Cloud_Computing\",\n",
        "    \"Unix\",\n",
        "    \"Windows\",\n",
        "    \"Teamwork\",\n",
        "    \"Organization\",\n",
        "]\n",
        "\n",
        "HOBBIES = [\n",
        "    \"Basketball\",\n",
        "    \"Tennis\",\n",
        "    \"Swimming\",\n",
        "    \"Running\",\n",
        "    \"Chess\",\n",
        "    \"Painting\",\n",
        "    \"Hand_Stand\",\n",
        "]\n",
        "\n",
        "PROTECTED = [\n",
        "    \"URM\",\n",
        "    \"Female\",\n",
        "    \"Disability\",\n",
        "]\n",
        "\n",
        "OTHER = [\n",
        "    \"Years_Experience\",\n",
        "    \"GPA\",\n",
        "    \"Prestigious_University\",\n",
        "]\n",
        "\n",
        "COLUMNS = [\"Interview\"] + PROTECTED + OTHER + SKILLS + HOBBIES\n",
        "\n",
        "SKILLS_AND_HOBBIES = SKILLS + HOBBIES\n",
        "FEATURES = SKILLS_AND_HOBBIES + OTHER + PROTECTED\n",
        "FEATURES_WITHOUT_PROTECTED = SKILLS_AND_HOBBIES + OTHER\n",
        "\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.metrics import ClassificationMetric\n",
        "import sklearn.model_selection\n",
        "\n",
        "def tuples_to_dict(t):\n",
        "  return {\n",
        "      \"Female\":[t[0]],\n",
        "      \"URM\":[t[1]],\n",
        "      \"Disability\":[t[2]]\n",
        "  }\n",
        "\n",
        "def get_aif_dataset(df, preds=None, scores=None):\n",
        "  preds_df = df.copy()\n",
        "  if preds is not None:\n",
        "    preds_df[\"Interview\"] = preds\n",
        "\n",
        "  dataset= BinaryLabelDataset(\n",
        "      df = preds_df,\n",
        "      label_names = [\"Interview\"],\n",
        "      protected_attribute_names = PROTECTED,\n",
        "  )\n",
        "\n",
        "  if scores is not None:\n",
        "    dataset.scores = scores.reshape(-1,1)\n",
        "  return dataset\n",
        "\n",
        "def get_metric(predictions, privileged_groups, unprivileged_groups):\n",
        "  dataset = BinaryLabelDataset(\n",
        "      df = data_test,\n",
        "      label_names = [\"Interview\"],\n",
        "      protected_attribute_names = PROTECTED,\n",
        "  )\n",
        "\n",
        "  preds_df = data_test.copy()\n",
        "  preds_df[\"Interview\"] = predictions\n",
        "\n",
        "  dataset_pred = BinaryLabelDataset(\n",
        "      df = preds_df,\n",
        "      label_names = [\"Interview\"],\n",
        "      protected_attribute_names = PROTECTED,\n",
        "  )\n",
        "\n",
        "  return ClassificationMetric(\n",
        "      dataset,\n",
        "      dataset_pred,\n",
        "      privileged_groups = [tuples_to_dict(t) for t in privileged_groups],\n",
        "      unprivileged_groups = [tuples_to_dict(t) for t in unprivileged_groups]\n",
        "  )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvK0qCa9Kcwg"
      },
      "source": [
        "# Recap: Last Time\n",
        "\n",
        "**Questions:**\n",
        "* What did we do last time?\n",
        "* Which models performed well in terms of accuracy?\n",
        "* What do the terms **statistical parity difference**, **false negative rate difference** and **false positive rate difference** mean?\n",
        "\n",
        "**Exercise:** Describe the **statistical parity difference**, and **false negative rate difference** and **false positive rate difference** in the following diagrams:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt2YgYUXLLwA"
      },
      "source": [
        "display(Image('metric_diagrams/Fairness Metrics.png'))\n",
        "display(Image('metric_diagrams/Fairness Metrics (1).png'))\n",
        "display(Image('metric_diagrams/Fairness Metrics (2).png'))\n",
        "display(Image('metric_diagrams/Fairness Metrics (3).png'))\n",
        "display(Image('metric_diagrams/Fairness Metrics (4).png'))\n",
        "display(Image('metric_diagrams/Fairness Metrics (5).png'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds3gxTU2LwX9"
      },
      "source": [
        "**Questions**:\n",
        "* How did we try to improve fairness metrics?\n",
        "* What were some tradeoffs that we needed to make?\n",
        "\n",
        "# Today\n",
        "\n",
        "Today we'll look at some more advanced techniques in machine learning fairness.\n",
        "\n",
        "As always, we start by loading in the data. We also copy the `report_fairness_metrics` function from the last notebook. Remind yourself of what it does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMZvAGQpLSLI"
      },
      "source": [
        "data = pd.read_csv(\"data.csv\")\n",
        "data_train, data_test = sklearn.model_selection.train_test_split(data, test_size=0.2, random_state=1)\n",
        "x_train = data_train[FEATURES]\n",
        "x_test = data_test[FEATURES]\n",
        "y_train = data_train[\"Interview\"]\n",
        "y_test = data_test[\"Interview\"]\n",
        "\n",
        "\n",
        "def return_fairness_metrics(predictions, privileged_groups, unprivileged_groups):\n",
        "  metrics = get_metric(predictions, privileged_groups, unprivileged_groups)\n",
        "  spd = metrics.statistical_parity_difference()\n",
        "  fnrd = metrics.false_negative_rate(privileged=True) - metrics.false_negative_rate(privileged=False)\n",
        "  fprd = metrics.false_positive_rate(privileged=False) - metrics.false_positive_rate(privileged=True)\n",
        "  aod = metrics.average_odds_difference()\n",
        "  acc = metrics.accuracy()\n",
        "  return spd, fnrd, fprd, aod, acc\n",
        "\n",
        "\n",
        "def report_fairness_metrics(predictions, privileged_groups, unprivileged_groups):\n",
        "  spd, fnrd, fprd, aod, acc = return_fairness_metrics(predictions, privileged_groups, unprivileged_groups)\n",
        "\n",
        "  print(\"Statistical parity difference: \", end=\"\")\n",
        "  print(round(spd, 3))\n",
        "  print(\"False negative rate difference: \", end=\"\")\n",
        "  print(round(fnrd, 3))\n",
        "  print(\"False positive rate difference: \", end=\"\")\n",
        "  print(round(fprd, 3))\n",
        "  print(\"Average odds difference: \", end=\"\")\n",
        "  print(round(aod, 3))\n",
        "  print(\"Accuracy: \", end=\"\")\n",
        "  print(round(acc, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auJLpJk_XgR_"
      },
      "source": [
        "Let's also train a Logistic Regression model on the data and remind ourselves how it does in terms of accuracy and fairness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Oc-U07KMWy_"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "report_fairness_metrics(lr.predict(x_test), [(0,0,0)], [(1,0,0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFxZk9aZq20c"
      },
      "source": [
        "# Reject Option Classfication\n",
        "\n",
        "Now that we have our model, we'd like to make it more fair. One way to make our predictions more fair is to leverage the probability/uncertainty interpretation of logistic regression. As you know, logistic regression outputs a score for each input which can be interpreted as a probability between 0 and 1.\n",
        "\n",
        "Run the next cell to see our model's predicted probabilities on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKs0ikU6q119"
      },
      "source": [
        "lr.predict_proba(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIGE7h_Tsh3x"
      },
      "source": [
        "**Questions**:\n",
        "* How do you interpret these valuies?\n",
        "* How do we get from these values to the actual predictions of the model?\n",
        "\n",
        "Because we are doing binary classification, a probablity of 1 means the candidate was offered an interview wherease that of 0 means that they weren't. Thus, it's enough to just look at the probability that someone gets an interview. We extract this information with the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZAEqWH-LCdc"
      },
      "source": [
        "lr.predict_proba(x_test)[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz_avNKTsy9q"
      },
      "source": [
        "## Thresholds and Certainty\n",
        "\n",
        "Here's an idea. Because our model is biased against the unprivileged group, **when the model is \"uncertain\", we could favor the unprivileged group**. For example if the model is uncertain about an applicant in the unprivileged group, give them an interview, however, if a model is uncertain about an applicant in the privileged group, don't give them an interview.\n",
        "\n",
        "The hope here is that this procedure counteracts the bias in the model.\n",
        "\n",
        "**Question:** How would you define when the logistic regression is uncertain?\n",
        "\n",
        "**Exercise**: Look at some of the predictions from before. Find one case where the model was **certain** and one where the model was **uncertain**.\n",
        "\n",
        "Let's remember some key definitions:\n",
        "* A **threshold** is a value between 0 and 1 such that when the model's predicted pobability exceeds the threshold, the data point is classified as positive. Typically just 0.5.\n",
        "* An **uncertainty margin** is how close to the threshold the predicted probability is to be considered 'uncertain'.\n",
        "\n",
        "\n",
        "**Exercise:** Use the definition of threshold and margin to write a function that converts from a probability to one of three predictions: \"Interview\", \"No Interview\" and \"Uncertain\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCiK1-csYPCw"
      },
      "source": [
        "def probability_to_prediction(p, threshold, margin):\n",
        "  ### BEGIN CODE HERE ###\n",
        "  # Replace TODO's with the correct condition\n",
        "  if \"TODO\":\n",
        "    return \"Interview\"\n",
        "  if \"TODO\":\n",
        "    return \"No Interview\"\n",
        "  else:\n",
        "    return \"Uncertain\"\n",
        "  ### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff3BOTeBY617"
      },
      "source": [
        "**Exercise**: Run the next cell to visualize the classifications. Play around with the threshold and margin to see how they change the predictions of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xoKAw8DFCdG"
      },
      "source": [
        "margin = 0.05\n",
        "threshold = 0.5\n",
        "\n",
        "\n",
        "def plot_predictions(margin, threshold):\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "  df[\"Actual Label\"] = y_test\n",
        "  df[\"Predicted Probability\"] = lr.predict_proba(x_test)[:, 1]\n",
        "\n",
        "\n",
        "  df[\"Prediction\"] = [probability_to_prediction(v, threshold, margin) for v in df[\"Predicted Probability\"]]\n",
        "\n",
        "  fig = px.scatter(df, x=\"Predicted Probability\", y=\"Actual Label\", color=\"Prediction\")\n",
        "  fig.add_scatter(x=[threshold, threshold], y=[0, 1], name=\"Threshold\")\n",
        "  fig.add_scatter(x=[min(1, threshold+margin), min(1, threshold+margin)], y=[0, 1], name=\"Uncertainty Upper Bound\")\n",
        "  fig.add_scatter(x=[max(0, threshold-margin), max(0, threshold-margin)], y=[0, 1], name=\"Uncertainty Lower Bound\")\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "plot_predictions(margin, threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaK6MP5_J9Fb"
      },
      "source": [
        "Using the idea from the previous paragraph, what happens to a candidate that is in the uncertainty range:\n",
        "* If they are privileged?\n",
        "* If they are unprivileged?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_0o_wihthKZ"
      },
      "source": [
        "### How does shifting the threshold affect the different fairness metrics?\n",
        "\n",
        "**Exercise**: In the next cell we'd like to visualize how changing the threshold of the model affects the fairness metrics. Complete the `plot_thresholds` function. Check out the comments in the code for more guidance.\n",
        "\n",
        "Hint: How do you convert predicted probabilities to predictions (Ignore the margin for now and set interview as 1 and no interview as 0)?\n",
        "\n",
        "Hint: Use the `return_fairness_metrics` function to get the fairness metrics of a set of predictions. Check the definition of this function in one of the earlier code cells. Use [(0,0,0)] as the privileged group and [(1, 0, 0)] as the unprivileged group.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onyEM4lt410-"
      },
      "source": [
        "def plot_threshold(predicted_probabilities):\n",
        "  \"\"\"\n",
        "  predicted_probabilities is a list of probability outputs from our model.\n",
        "  \"\"\"\n",
        "  thresholds = np.arange(0, 1, 0.02) # A list of thresholds we'd like to test\n",
        "  rows = []\n",
        "\n",
        "  for t in thresholds:\n",
        "    ### BEGIN CODE HERE ###\n",
        "    \"\"\"\n",
        "      TODO: Fill in the rows variable with a row corresponding to the\n",
        "      fairness metrics of each threshold. Each row should have\n",
        "      the threshold, the statistical parity difference, the\n",
        "      fnrd, fprd, the average odds difference and the accuracy\n",
        "      in that order.\n",
        "\n",
        "      By the end of this for loop, rows should look like this:\n",
        "      [\n",
        "        (t, spd, fnrd, fprd, aod, acc),\n",
        "        (t, spd, fnrd, fprd, aod, acc),\n",
        "        (t, spd, fnrd, fprd, aod, acc),...\n",
        "      ]\n",
        "    \"\"\"\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "  df = pd.DataFrame(rows, columns=[\"Threshold\", \"Statistical Parity Difference\", \"FNRD\", \"FPRD\", \"Average Odds Difference\", \"Accuracy\"])\n",
        "  df = df.melt(id_vars='Threshold',var_name='Metric', value_name='Value')\n",
        "  fig = px.line(df, x=\"Threshold\", y=\"Value\", color=\"Metric\")\n",
        "  # fig.add_scatter(x=predicted_probabilities, y=np.zeros(predicted_probabilities.shape), mode='markers')\n",
        "  fig.show()\n",
        "\n",
        "plot_threshold(lr.predict_proba(x_test)[:, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnarz2FjLHHR"
      },
      "source": [
        "**Exercise:** By visual inspection, identify a threshold that acheives better fairness metrics than the standard 0.5 without sacrificing much of the accuracy.\n",
        "\n",
        "**Question:** What happens when the threshold is close to 0 or close to 1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T84j3FjoK-OR"
      },
      "source": [
        "The **Reject Option Classification** algorithm is a brute force search over different thresholds and certainty margins.\n",
        "\n",
        "We have some fairness metric that we want to be within a certain range (between `metric_lb`, and `metric_ub`). Then we simply search many different thresholds and certainty margins while applying our \"favor the unprivileged group\" strategy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYZd3VFCQO4A"
      },
      "source": [
        "from aif360.algorithms.postprocessing import RejectOptionClassification\n",
        "\n",
        "ROC = RejectOptionClassification(\n",
        "    unprivileged_groups= [{\"Female\":1, \"URM\":0, \"Disability\":0}],\n",
        "    privileged_groups= [{\"Female\":0, \"URM\":0, \"Disability\":0}],\n",
        "    metric_name=\"Statistical parity difference\",\n",
        "    metric_ub=0.05, metric_lb=-0.05\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziuJ9-coXgnv"
      },
      "source": [
        "dataset_true = get_aif_dataset(data_test)\n",
        "dataset_pred = get_aif_dataset(data_test, scores=lr.predict_proba(x_test)[:, 1])\n",
        "dataset_transformed = ROC.fit_predict(dataset_true, dataset_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeSqopacKhln"
      },
      "source": [
        "Run the next cell to see which threshold and margin the algorithm found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAKY0kmAJ5Zx"
      },
      "source": [
        "print(\"Optimal classification threshold (with fairness constraints) = \", ROC.classification_threshold)\n",
        "print(\"Optimal ROC margin = \",  ROC.ROC_margin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G8BSVWj7mp2"
      },
      "source": [
        "**Exercise**: Use the `plot_predictions` function on the margin and the threshold value we found to visualize our predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiTZ4jmv7wMC"
      },
      "source": [
        " ### BEGIN CODE HERE ###\n",
        " ### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzOuJa6SKz9D"
      },
      "source": [
        "How do our new datapoints do?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnciGqLZXLGP"
      },
      "source": [
        "report_fairness_metrics(dataset_transformed.labels, privileged_groups=[(0,0,0)], unprivileged_groups=[(1, 0, 0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op2pW0ngKoyf"
      },
      "source": [
        "**Exercise**: Experiment with different fairness metrics and bounds. Check [here](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html#aif360.algorithms.postprocessing.RejectOptionClassification) for the possible fairness metrics we can optimize over."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU5Voux7vgY7"
      },
      "source": [
        "# Reweighing\n",
        "\n",
        "**Reject option classification** (previous section) lets us adapt our existing logistic regression model to satisfy some fairness constraints. Importantly, it doesn't change the underlying model at all. Thus, it is called a **postprocessing** algorithm. One drawback of this approach is that although we are guaranteed to have our fairness metric within a certain bound, we might sacrifice a lot of accuracy.\n",
        "\n",
        "Since we are developing our algorithm in-house, we actually have more control.\n",
        "In fact, **since we have access to the data (which is just our previous hiring patterns), we can change the data itself**! Algorithms that do this are known as **preprocessing** algorithms.\n",
        "\n",
        "There's a common saying in machine learning called \"Garbage in, garbage out\" refering to the case where if you feed a model \"garbage\" input, you're going to get a \"garbage model\". Thus, having good data and preprocessing techniques are very important.\n",
        "\n",
        "In the **reweighing** algorithm, we assign a weight to each datapoint in order to debias the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xNHgMtuUJOV"
      },
      "source": [
        "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
        "RW = Reweighing(\n",
        "    unprivileged_groups= [{\"Female\":1, \"URM\":0, \"Disability\":0}],\n",
        "    privileged_groups= [{\"Female\":0, \"URM\":0, \"Disability\":0}],\n",
        ")\n",
        "\n",
        "dataset_orig_train = get_aif_dataset(data_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BNN-PL9Zq_h"
      },
      "source": [
        "RW.fit(dataset_orig_train)\n",
        "dataset_transf_train = RW.transform(dataset_orig_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH_MFkj_3_LK"
      },
      "source": [
        "dataset_transf_train.instance_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83l0cr673-_D"
      },
      "source": [
        "**Question**: How do you think the weights are chosen?\n",
        "\n",
        "Since we want our model to be unbiased, we want our data to be unbiased. Thus, in the reweighing algorithm, we find weights that force the **weighted discrimination** to be 0.\n",
        "\n",
        "**Optional:** Mathematical Details\n",
        "\n",
        "Discrimination is just another name for the statistical parity difference which is:\n",
        "\n",
        "$$ \\text{ (Interview rate for the unprivileged group) - (Interview rate for the privileged group)}$$\n",
        "\n",
        "We can rewrite this equation as\n",
        "\n",
        "$$\n",
        "\\left(\\frac{\\text{Number of interviewed, unprivileged applicants}}{\\text{Number of unprivileged applicants}}\\right) - \\left(\\frac{\\text{Number of interviewed, privileged applicants}}{\\text{Number of privileged applicants}}\\right)\n",
        "$$\n",
        "\n",
        "as rewrite again as\n",
        "\n",
        "$$\n",
        "\\left(\\frac{\\sum_{x : x \\text{ is unprivileged and got an interview}} 1 }{\\sum_{x : x \\text{ is unprivileged}} 1 } \\right) - \\left(\\frac{\\sum_{x : x \\text{ is privileged and got an interview}} 1 }{\\sum_{x : x \\text{ is privileged}} 1 }\\right)\n",
        "$$\n",
        "\n",
        "Here the sum notation is just adding a 1 for each person that satisfies the condition.\n",
        "\n",
        "**Exercise:** Convince yourselves that these equations all refer to the same thing.\n",
        "\n",
        "The weighted discrimination is then the following:\n",
        "$$\n",
        "\\left(\\frac{\\sum_{x : x \\text{ is unprivileged and got an interview}} W(x) }{\\sum_{x : x \\text{ is unprivileged}}  W(x) } \\right) - \\left(\\frac{\\sum_{x : x \\text{ is privileged and got an interview}}  W(x) }{\\sum_{x : x \\text{ is privileged}}  W(x) }\\right)\n",
        "$$\n",
        "\n",
        "Where $W(x)$ is the weight for a certain applicant.\n",
        "\n",
        "Let's get some intuition for this. Let's say there is discrimintation in the dataset, and the interview rate for the privileged group is higher than the interview rate for the unprivileged group. Since we want the weighted discrimination to be 0, we need to either lower the interview rate for the privileged group or increase the interview rate for the unprivileged group. Let's imagine for now, all the weights are set to 1.\n",
        "\n",
        "**Questions:**\n",
        "* What happens to the equation if we increase the weight for an unprivileged applicant who gets an interview?\n",
        "* What happens to the equation if we decrease the weight for an unprivileged applicant who doesn't get an interview?\n",
        "\n",
        "Luckily, solving an equation let's us find these weights easily. Check [this](https://en.wikipedia.org/wiki/Fairness_(machine_learning)#Reweighing) for more details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnyZnpqL95Wc"
      },
      "source": [
        "Once we have our weights, we can then train our model with an emphasis on datapoints with higher weights.\n",
        "\n",
        "**Exercise**: Train a new LogisticRegression model on the training data with the weights found by the Reweighing algorithm.\n",
        "\n",
        "Then evaluate the fairness and accuracy of this model using `report_fairness_metrics`\n",
        "\n",
        "Hint: The `class_weight` parameter in the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) might be useful!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g13X0Xed8PdV"
      },
      "source": [
        "### BEGIN CODE HERE ###\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBSn0rNV-juI"
      },
      "source": [
        "**Question:** How did our model do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJfFBajJvgmA"
      },
      "source": [
        "# Optional: Exponentiated Gradient Reduction\n",
        "\n",
        "\n",
        "So far we talked about **postprocessing** algorithms which take an existing model and reinterpret the outputs. These can guarantee good performance on fairness metrics. However, we sacrifice some accuracy. We also talked about a **preprocessing** algorithm which attempts to promote fairness by changing the training data to be less biased in the first place. This approach worked out pretty well, acheiving high accuracy while also attaining good performance on fairness metrics.\n",
        "\n",
        "There is another type of algorithm called an **inprocessing** algorithm. Instead of working with the outputs of the model (like postprocessing algorithms) or the inputs to the model (like preprocessing algorithms), *inprocessing algorithms work with the training and optimization of the model* which gives an additional layer of flexibility.\n",
        "\n",
        "**Exponentiated Gradient Reduction** is an algorithm that factors fairness into the optimization of the algorithms. At a high level you can think of fairness being factored into the loss function. Check out the [original paper](https://arxiv.org/pdf/1803.02453.pdf) for some *very* advanced extra reading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRmgHZvZAjyn"
      },
      "source": [
        "from aif360.algorithms.inprocessing.exponentiated_gradient_reduction import ExponentiatedGradientReduction\n",
        "\n",
        "\n",
        "estimator = LogisticRegression()\n",
        "exp_grad_red = ExponentiatedGradientReduction(estimator=estimator,\n",
        "                                              constraints=\"EqualizedOdds\",drop_prot_attr=False)\n",
        "exp_grad_red.fit(dataset_orig_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8xwrcqkCICp"
      },
      "source": [
        "exp_grad_red_pred = exp_grad_red.predict(get_aif_dataset(data_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T888a8cgCZaY"
      },
      "source": [
        "report_fairness_metrics(exp_grad_red_pred.labels, privileged_groups=[(0,0,0)], unprivileged_groups=[(1, 0, 0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1XgC2iYwJT0"
      },
      "source": [
        "# Conclusion\n",
        "Congratulations! You have completed the project! Feel good about what you have done. You have saved your company a lot of time all the while being extremely sensitive and cognizant of fairness and bias - something that is often overlooked.\n",
        "\n",
        "Make sure to go back to carefully pick the best model. Once you are satisfied with the accuracy and fairness of your model it's time to present your findings and results to your bosses at Rayo Tech (or in reality your peers and parents and other instructors)!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xgHl8k82YZe"
      },
      "source": [
        "# Extra: Fairness in the Real World - An open ended exploration\n",
        "\n",
        "Check out [this demo](https://aif360.mybluemix.net/data) from IBM on some of the algorithms and metrics that we learned about applied to real world datasets.\n",
        "\n",
        "To test yourself and reinforce this knowledge, we have provided you with a new dataset - this time from the real world! Check out the next notebook to explore this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg93KN2jp2X_"
      },
      "source": [
        "# Acknowledgements\n",
        "* Data and notebook by Harry Sha. Email harryshahai@gmail.com for bugs/questions!\n",
        "* The [AI Fairness 360 Library](https://aif360.readthedocs.io/en/latest/) which implements the different fairness metrics and algorithms.\n"
      ]
    }
  ]
}